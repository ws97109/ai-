import json
import os
import re
from tools.LLM.ollama_agent import *
# ä¿®æ”¹å½“å‰å·¥ä½œç›®å½•
os.chdir('../')


# æ–¹å¼ä¸€

api_url = "http://127.0.0.1:11434/api"
ollama_agent = OllamaAgent("qwen2.5:7b", api_url, "agent_chat")


# æ–¹å¼äºŒ

# from tools.LLM.modelscope_agent import ModaAgent
# ollama_agent = ModaAgent()


#æ–¹å¼ä¸‰

# from tools.LLM.qwen_turbo_agent import *
# ollama_agent = QwenTurboAgent()


#æ–¹å¼å›› deepseek-v3

# from tools.LLM.deepseek_agent import DeepSeekAgent
# ollama_agent = deepsk = DeepSeekAgent()



can_go_place = ['åŒ»é™¢', 'å’–å•¡åº—', 'èœœé›ªå†°åŸ', 'å­¦æ ¡', 'å°èŠ³å®¶', 'ç«é”…åº—', 'ç»¿é“', 'å°æ˜å®¶', 'å°ç‹å®¶', 'è‚¯å¾·åŸº',
                    'ä¹¡æ‘åŸº', 'å¥èº«æˆ¿', 'ç”µå½±é™¢', 'å•†åœº', 'æµ·è¾¹']



# æ¯æ—¥è®¡åˆ’è¡¨
def run_gpt_prompt_generate_hourly_schedule(persona,now_time):
    def __func_clean_up(gpt_response):

        cr = gpt_response
        return cr

    def __func_validate(gpt_response):
        try:
            gpt_response = gpt_response.replace("```","").split("json")[1][1:]
            gpt_response = json.loads(gpt_response.strip('\n'))['output']
            total_time = sum(item[1] for item in gpt_response)
            # print(total_time)
            if total_time > 1920:
                return False
            __func_clean_up(gpt_response)
        except:
            return False
        return True

    generate_prompt = OllamaAgent.generate_prompt(
        [persona,now_time],
        r"aiå°é®åŸå§‹æª”/tools/LLM/prompt_template/ç”Ÿæˆæ—¥ç¨‹å®‰æ’æ—¶é—´è¡¨.txt")
    output = ollama_agent.ollama_safe_generate_response(generate_prompt, "", "ä½ ä¸éœ€è¦è°ƒæ•´ï¼Œåªéœ€è¦ç»™æˆ‘è¾“å‡ºä¸€ä¸ªæœ€ç»ˆçš„ç»“æœï¼Œæˆ‘éœ€è¦ä¸€ä¸ªæ ‡å‡†çš„æ•°ç»„æ ¼å¼", 5,
                                                        __func_validate, __func_clean_up)
    # print("run_gpt_prompt_generate_hourly_schedule",output)
    if "json" in output:
        output = output.replace("```", "").split("json")[1][1:]
        output = json.loads(output.strip('\n'))['output']
        return output

    else:
        # print(output)
        return output


# æ¯å¤©è‹é†’æ—¶é—´
def run_gpt_prompt_wake_up_hour(persona,now_time,hourly_schedule):
    def __func_clean_up(gpt_response):
        cr = gpt_response
        return cr

    def __func_validate(gpt_response):
        try:
            if "output" in gpt_response:
                pattern = r'"output"\s*:\s*"([^"]+)"'
                match = re.search(pattern, gpt_response)
                output_value = match.group(1)
                __func_clean_up(output_value)
            else:
                return False
        except:
            return False
        return True
    generate_prompt = OllamaAgent.generate_prompt(
        [persona,now_time,hourly_schedule],
        r"aiå°é®åŸå§‹æª”/tools/LLM/prompt_template/èµ·åºŠæ—¶é—´.txt")
    output = ollama_agent.ollama_safe_generate_response(generate_prompt, "",
                                                        "åªéœ€è¦ç»™æˆ‘è¾“å‡ºä¸€ä¸ªæœ€ç»ˆçš„ç»“æœä¸éœ€è¦ç»™æˆ‘å…¶ä»–ä»»ä½•ä¿¡æ¯ï¼Œæˆ‘éœ€è¦ä¸€ä¸ªæ ‡å‡†çš„æ—¥æœŸæ ¼å¼ï¼Œæ¯”å¦‚ï¼š07-01ï¼ˆè¡¨ç¤ºæ—©ä¸Šä¸ƒç‚¹é›¶ä¸€åˆ†èµ·åºŠï¼‰",
                                                        3,
                                                        __func_validate, __func_clean_up)
    pattern = r'"output"\s*:\s*"([^"]+)"'
    match = re.search(pattern, output)
    output = match.group(1)
    return output

# è¡ŒåŠ¨è½¬è¡¨æƒ…
def run_gpt_prompt_pronunciatio(Action_dec):
    def __chat_func_clean_up(gpt_response):
        cr = gpt_response.strip()
        if len(cr) > 3:
            cr = cr[:2]
        if len(cr) == 0:
            cr = 'ğŸ˜´ğŸ’¤'
        return cr
    def __chat_func_validate(gpt_response):  
        try:
            if "output" in gpt_response:
                pattern = r'"output"\s*:\s*"([^"]+)"'
                match = re.search(pattern, gpt_response)
                output_value = match.group(1)
                __chat_func_clean_up(output_value)
            else:
                return False
        except:
            print("__chat_func_validate exception")
            return False
        return True
    example_output = "ğŸ›ğŸ§–â€â™€ï¸"  ########
    special_instruction = "è¾“å‡ºåªåŒ…å«è¡¨æƒ…ç¬¦å·"  ########
    generate_prompt = OllamaAgent.generate_prompt(
        [Action_dec],
        r"aiå°é®åŸå§‹æª”/tools/LLM/prompt_template/è¡Œä¸ºè½¬ä¸ºå›¾æ ‡æ˜¾ç¤º.txt")
    output = ollama_agent.ollama_safe_generate_response(generate_prompt, example_output, special_instruction, 5,__chat_func_validate,__chat_func_clean_up,'{"output":"ğŸ˜´ğŸ’¤"}')
    pattern = r'"output"\s*:\s*"([^"]+)"'
    match = re.search(pattern, output)
    output = match.group(1)
    return output


# ä¸¤ä¸ªæ™ºèƒ½ä½“é—´çš„å¯¹è¯
def double_agents_chat(maze,agent1_name,agent2_name,curr_context,init_summ_idea,target_summ_idea,now_time):
    def __chat_func_clean_up(gpt_response):
        try:
            output_value = gpt_response
        except:
            output_value = ""
        return output_value

    def __chat_func_validate(gpt_response):
        # print(type(gpt_response))
        try:
            if "json" in gpt_response:
                gpt_response = gpt_response.replace("```", "").split("json")[1][1:]
                gpt_response = json.loads(gpt_response.strip('\n'))['output']
                __chat_func_clean_up(gpt_response)
            else:
                pattern = r'"output"\s*:\s*"([^"]+)"'
                match = re.search(pattern, gpt_response)
                __chat_func_clean_up(match)
        except:
            return False
        return True

    generate_prompt = OllamaAgent.generate_prompt(
        [maze,agent1_name, agent2_name, curr_context, init_summ_idea, target_summ_idea,now_time], r"aiå°é®åŸå§‹æª”/tools/LLM/prompt_template/èŠå¤©.txt")

    example_output = '[["ä¸¹å°¼", "ä½ å¥½"], ["è‹å…‹", "ä½ ä¹Ÿæ˜¯"] ... ]'
    special_instruction = 'è¾“å‡ºåº”è¯¥æ˜¯ä¸€ä¸ªåˆ—è¡¨ç±»å‹ï¼Œå…¶ä¸­å†…éƒ¨åˆ—è¡¨çš„å½¢å¼ä¸º[â€œ<åå­—>â€ï¼Œâ€œ<è¯è¯­>â€]ã€‚'

    output = ollama_agent.ollama_safe_generate_response(generate_prompt, example_output, special_instruction, 5,__chat_func_validate,__chat_func_clean_up,'''{"output":"[['å°æ˜', 'æ˜å¤©å»è‚¯å¾·åŸºå—'], ['å°èŠ³', 'å¥½çš„ï¼Œæ¯å¤©ä¸Šåˆåä¸€ç‚¹åœ¨è‚¯å¾·åŸºé›†åˆ']]"}''')
    if "json" in output:
        output = output.replace("```", "").split("json")[1][1:]
        output = json.loads(output.strip('\n'))['output']
        return output

    else:
        pattern = r'"output"\s*:\s*"([^"]+)"'
        match = re.search(pattern, output)
        output = match.group(1)
        return output


# åˆ¤æ–­åšè¿™ä»¶äº‹æƒ…éœ€è¦å»å“ªä¸ªåœ°æ–¹
def go_map(agent_name, home , curr_place, can_go, curr_task):
    def __chat_func_clean_up(gpt_response):  
        return gpt_response

    def __chat_func_validate(gpt_response):  
        try:
            if "output" in gpt_response:
                pattern = r'"output"\s*:\s*"([^"]+)"'
                match = re.search(pattern, gpt_response)
                output_value = match.group(1)
                __chat_func_clean_up(output_value)
            else:
                return False
        except:
            return False
        return True

    example_output = 'æµ·è¾¹'
    special_instruction = ''

    generate_prompt = OllamaAgent.generate_prompt(
        [agent_name,home , curr_place, can_go, curr_task],
        r"aiå°é®åŸå§‹æª”/tools/LLM/prompt_template/è¡ŒåŠ¨éœ€è¦å»çš„åœ°æ–¹.txt")

    output = ollama_agent.ollama_safe_generate_response(generate_prompt, example_output, special_instruction, 3,__chat_func_validate,__chat_func_clean_up)
    pattern = r'"output"\s*:\s*"([^"]+)"'
    match = re.search(pattern, output)
    output = match.group(1)
    return output


# æ€è€ƒæ”¹å˜æ—¥ç¨‹å®‰æ’
def modify_schedule(old_schedule,now_time,memory,wake_time,role_xg):
    def __func_clean_up(gpt_response):
        cr = gpt_response
        return cr

    def __func_validate(gpt_response):
        try:
            gpt_response = gpt_response.replace("```","").split("json")[1][1:]
            gpt_response = json.loads(gpt_response.strip('\n'))['output']
            # qwen2.5-7bå›ç­”å°‘ä¸€ä¸ª]å¯¼è‡´æ— æ³•å¯¹è±¡åŒ–ä¸ºåˆ—è¡¨
            if type(gpt_response) == str:
                gpt_response = gpt_response.replace("'", '"')
                gpt_response_test = json.loads(gpt_response)
            __func_clean_up(gpt_response)
        except:
            return False
        return True

    generate_prompt = OllamaAgent.generate_prompt(
        [old_schedule,now_time,memory,wake_time,role_xg],
        r"aiå°é®åŸå§‹æª”/tools/LLM/prompt_template/ç»†åŒ–æ¯æ—¥å®‰æ’æ—¶é—´è¡¨.txt")
    output = ollama_agent.ollama_safe_generate_response(generate_prompt, "", "ä½ ä¸éœ€è¦è°ƒæ•´ï¼Œåªéœ€è¦ç»™æˆ‘è¾“å‡ºä¸€ä¸ªæœ€ç»ˆçš„ç»“æœï¼Œæˆ‘éœ€è¦ä¸€ä¸ªæ ‡å‡†çš„æ•°ç»„æ ¼å¼", 10,
                                                        __func_validate, __func_clean_up)
    # print("modify_schedule",output)
    if type(output) in [str]:
        if "json" in output:
            output = output.replace("```", "").split("json")[1][1:]
            output = json.loads(output.strip('\n'))['output']
            if type(output) == str:
                output = output.replace("'", '"')
                output = json.loads(output)

            return output
        else:
            output = json.loads(output)
            return output
    else:
        return output


# æ€»ç»“ä»Šå¤©çš„ä¸€åˆ‡å†™å…¥è®°å¿†æ–‡ä»¶
def summarize(memory,now_time,name):
    def __chat_func_clean_up(gpt_response):  
        return gpt_response

    def __chat_func_validate(gpt_response):  
        try:
            __chat_func_clean_up(gpt_response)
        except:
            return False
        return True
    generate_prompt = OllamaAgent.generate_prompt(
        [memory,now_time,name],
        r"aiå°é®åŸå§‹æª”/tools/LLM/prompt_template/æ€»ç»“ç»å†äº¤è°ˆä¸ºè®°å¿†.txt")
    example_output = ''
    special_instruction = ''
    output = ollama_agent.ollama_safe_generate_response(generate_prompt, example_output, special_instruction, 3,
                                                   __chat_func_validate, __chat_func_clean_up)

    # print('summarize',output)
    return output


